#!/usr/bin/python
'''
Copyright (C) 2003-2004 Douglas Thain and the University of Wisconsin
Copyright (C) 2005- The University of Notre Dame
This software is distributed under the GNU General Public License.
See the file COPYING for details.
'''
import sys
from stat import *
from pprint import pprint
import subprocess
import platform
import re
import tarfile
import StringIO
from optparse import OptionParser
import os
import hashlib
import difflib
import sqlite3
import shutil
import datetime
import time
import getpass
import grp

cvmfs_parrot = 0
fake_passwd = 0
fake_group = 0
parrot_localfile_flag = 0
need_create_rootfs = 0
sandbox_no = 0
env_file = ""	
common_mountfile = ""
special_files = ""
hardware_platform = ""
os_type = ""
linux_distro = ''
host_linux_distro = ''
dist_name = ''
dist_version = ''
mount_dict = {}
task_path = ""
condor_submit_path = ""
parrot_submit_path = ""
task_file = ""
condor_submit_file = ""
parrot_submit_file = ""
condor_requirements = ""
cwd_setting = ""
user_cmd = ""
local_cctools_path = ""

if sys.version_info >= (3,):
	import urllib.request as urllib2
	import urllib.parse as urlparse
else:
	import urllib2
	import urlparse

if sys.version_info > (2,6,):
	import json
else:
	import simplejson as json #json module is introduce in python 2.4.3

""" Execute a command and return the standard output.
Parameter cmd: the command needs to execute using the subprocess module
Return: the output of the execution.
"""
def func_call(cmd):
	p = subprocess.Popen(cmd, stdout = subprocess.PIPE, shell = True)
	(output, err) = p.communicate()
	p_status = p.wait()
	return output

def func_call_output(cmd, output_dir):
	output = func_call(cmd)
	if not os.path.exists(output_dir):
		os.makedirs(output_dir)
	stdout = output_dir + "/stdout"
	with open(stdout, 'w+') as file1:
		file1.write(output)
	
def func_call_output_withenv(cmd, env_dict, output_dir):
	p = subprocess.Popen(cmd, env = env_dict, stdout = subprocess.PIPE, shell = True)
	(output, err) = p.communicate()
	p_status = p.wait()
	if not os.path.exists(output_dir):
		os.makedirs(output_dir)
	stdout = output_dir + "/stdout"
	with open(stdout, 'w+') as file1:
		file1.write(output)

""" Calculate the md5sum of a file
Parameter filename: the name of the file 
Parameter block_size: the size of each block
Return: the md5 value of the content of the file
"""
def md5_cal(filename, block_size=2**20):
	try:
		with open(filename, 'rb') as f:
			md5 = hashlib.md5()
			while True:
				data = f.read(block_size)
				if not data:
					break
				md5.update(data)
			return md5.hexdigest()
	except:
		sys.exit("md5_cal(" + filename + ") failed.\n")

""" Check whether a url is broken or not.
Parameter url: a url
Return: if the url is broken, return 1; otherwise, return 0.
"""
def url_check(url):
	try:
		f = urllib2.urlopen(urllib2.Request(url))
	except:
		print "url({0}) is broken.".format(url)
		return 1
	return 0

""" Download each dependency from the url and verify its integrity.
Parameter url: the storage location of the dependency
Parameter checksum: the checksum of the dependency
Parameter checksum_tool: the tool used to calculate the checksum, such as md5sum.
Parameter dest: the destination of the dependency where the downloaded dependency will be put
Parameter format_remote_storage: the file format of the dependency, such as .tgz.
"""
def download_dependency(url, checksum, checksum_tool, dest, format_remote_storage):
	print "Download software from %s into the dir (%s)" % (url, dest)
	dir_dest = os.path.dirname(dest)
	old_dest = dest #old_dest is the path of the uncompressed-version dependency

	scheme, netloc, path, query, fragment = urlparse.urlsplit(url)
	filename = os.path.basename(path)
	dest = os.path.join(dir_dest, filename) #dest is the path of the compressed-version dependency

	if not os.path.exists(dir_dest):
		os.makedirs(dir_dest)

	if not os.path.exists(dest):
		#download the dependency from the url
		#this method currently will fail when the data size is larger than the memory size, use subprocess + wget can solve it
		try:
			response = urllib2.urlopen(url)
			data = response.read()
		except urllib2.URLerror as e:
			sys.exit("URLerror({0}): {1}".format(e.errno, e.strerror))
		with open(dest, "wb") as code:
			code.write(data)
		#if the compressed-version dependency does not exist, the uncompressed-version directory will be deleted first
		if os.path.exists(old_dest):
			shutil.rmtree(old_dest)

	#calculate the checkusm of the compressed-version dependency
	if checksum_tool == "md5sum":
		local_checksum = md5_cal(dest)
	else:
		sys.exit(checksum_tool + "is not supported currently!")

	if not local_checksum == checksum:
		sys.exit("the version of " + url + " is incorrect!\n")
	
	#if the uncompressed-version dependency does not exist, uncompress the dependency
	if not os.path.exists(old_dest):
		tfile = tarfile.open(dest, "r:gz")
		tfile.extractall(dir_dest)

def cctools_download(sandbox_dir, sw_conn_cursor):
	cctools_item = db_search("cctools", host_linux_distro, hardware_platform, sw_conn_cursor) 
	dest = os.path.dirname(sandbox_dir) + "/cache/cctools-" + hardware_platform + "-" + host_linux_distro
	name = "cctools"
	print "dependency_process(%s) start: %s" % (name, datetime.datetime.now())
	download_dependency(cctools_item[3], cctools_item[6], "md5sum", dest, "tgz")
	print "dependency_process(%s) end: %s" % (name, datetime.datetime.now())
	return dest

#the db table itself does not understand the collaberation relationship between cvmfs, parrot, SITECONF
def cvmfs_software(item, sw_conn_cursor, sandbox_dir, batch_type):
	global cvmfs_parrot 
	cvmfs_parrot = 1
	#construct the parrot submit script
	parrot_submit_file.write("#!/bin/sh\n")
	parrot_submit_file.write("export HTTP_PROXY=http://cache01.hep.wisc.edu:3128\n")
	#download cctools
	cctools_item = db_search("cctools", host_linux_distro, hardware_platform, sw_conn_cursor) 
	dest = os.path.dirname(sandbox_dir) + "/cache/cctools-" + hardware_platform + "-" + host_linux_distro
	download_dependency(cctools_item[3], cctools_item[6], "md5sum", dest, "tgz")
	cvmfs_repo = item[3][6:]
	#print cvmfs_repo
	if cvmfs_repo == "cms.cern.ch":
		task_file.write('#!/bin/sh\n')
		task_file.write('rm -rf ${CMS_VERSION}\n\n')
		task_file.write('. /cvmfs/cms.cern.ch/cmsset_default.sh\n')
		task_file.write('scramv1 project CMSSW ${CMS_VERSION}\n')
		task_file.write('cd ${CMS_VERSION}\n')
		task_file.write('eval `scram runtime -sh`\n')
		task_file.write('cd ..\n')
		if cwd_setting == '':
			task_file.write('pwd; ' + user_cmd[0] + '\n')
		else:
			task_file.write('cd ' + cwd_setting + '; ' + user_cmd[0] + '\n')
		parrot_submit_file.write("export CMS_VERSION=" + item[1] + "\n")
		parrot_submit_file.write("export SCRAM_ARCH=" + item[2] + "\n")
		#download SITECONF
		site_item = db_search("cms-siteconf-local-cvmfs", "", "", sw_conn_cursor)
		dest = os.path.dirname(sandbox_dir) + "/cache/cvmfs/cms.cern.ch/SITECONF"
		download_dependency(site_item[3], site_item[6], "md5sum", dest, "tgz")
		parrot_dest = "%s/cache/cctools-%s-%s/bin/parrot_run" % (os.path.dirname(sandbox_dir), hardware_platform, host_linux_distro)
		#parrot_submit_file.write(parrot_dest + " --cvmfs-alien-cache=/tmp/hmeng -M /cvmfs/cms.cern.ch/SITECONF/local=" + dest + "/local /bin/sh " + sandbox_dir + "/task.sh\n" )
		#parrot_submit_file.write('env; ' + parrot_dest + " -d all -o log -M /cvmfs/cms.cern.ch/SITECONF/local=" + dest + "/local /bin/sh " + sandbox_dir + "/task.sh\n" )
		parrot_submit_file.write('env; ' + parrot_dest + " -M /cvmfs/cms.cern.ch/SITECONF/local=" + dest + "/local /bin/sh " + sandbox_dir + "/task.sh\n" )

		global parrot_localfile_flag
		parrot_localfile_flag = 1

""" Search a software given the name, version, platform information
"""
def db_search(name, version, platform, db_conn_cursor):
	sql_cmd = 'SELECT * FROM sw_table where name = "' + name + '" and version = "' + version + '" and platform = "' + platform + '"'
	db_conn_cursor.execute(sql_cmd)
	result = db_conn_cursor.fetchall()
	if len(result) == 0:
		sys.exit(sql_cmd + " fails!\n")
	#if multiple items satisfy the requirements, the first one will be used.
	item = result[0]
	return item

def dependency_process(name, version, platform, mountpoint, sw_conn_cursor, sandbox_dir, batch_type):
	item = db_search(name, version, platform, sw_conn_cursor)
	#table schema: (name text, version text, platform text, store text, store_type text, type text)
	store = item[3] 
	global mount_dict
	if store[0:5] == "cvmfs":
		cvmfs_software(item, sw_conn_cursor, sandbox_dir, batch_type)	
		mount_dict[mountpoint] = mountpoint
	else:
		if item[5] == 'os':
			dest = os.path.dirname(sandbox_dir) + "/cache/" + item[0] + '-' + item[1] + '-' + item[2]
			global env_file
			global common_mountfile
			global special_files
			global need_create_rootfs
			env_file = dest + "/env_list"
			common_mountfile = dest + "/common-mountlist"
			special_files = dest + "/special_files"
			need_create_rootfs = 1
		else:
			dest = os.path.dirname(sandbox_dir) + "/cache/" + item[0] + '-' + item[2] + '-' + item[1]
		download_dependency(store, item[6], "md5sum", dest, "tgz")
		mount_dict[mountpoint] = dest
		
def initialize_execution_environment_parameters(hardware_spec, kernel_spec, os_spec):
	global hardware_platform
	global os_type
	global dist_name
	global dist_version
	global linux_distro
	hardware_platform = hardware_spec["platform"].lower()
	os_type = kernel_spec["type"].lower()
	dist_name = os_spec["name"].lower()
	dist_version = os_spec["version"].lower()
	index = dist_version.find('.')
	linux_distro = dist_name + dist_version[:index]

def execution_environment_check(sandbox_dir, batch_type):
	print "Execution environment checking ..."
	
	global task_path
	global parrot_submit_path
	global task_file
	global parrot_submit_file
	task_path = sandbox_dir + "/task.sh"
	parrot_submit_path = sandbox_dir + "/local.sh"
	task_file = open(task_path, "wb")
	parrot_submit_file = open(parrot_submit_path, "wb")

	if batch_type != "docker" and batch_type != "chroot" and batch_type != "local":
		sys.exit("Currently local execution engine only support three sandbox techniques: docker, chroot or local!\n")

	uname_list = platform.uname() #(system,node,release,version,machine,processor)
#	for i in range(len(uname_list)):
#		print uname_list[i],
#	print ''
	
	linux_dist_list = platform.dist()
	print "The hardware information of the local machine:", linux_dist_list
	global host_linux_distro
	arch_index = uname_list[2].find('ARCH')
	if arch_index != -1:
		host_linux_distro = 'arch'		
	redhat_index = uname_list[2].find('el')
	centos_index = uname_list[2].find('centos')
	if redhat_index != -1:
		uname_dist_version = uname_list[2][redhat_index + 2]
		if centos_index != -1 or linux_dist_list[0].lower() == 'centos':
			host_linux_distro = 'centos' + uname_dist_version
		else:
			host_linux_distro = 'redhat' + uname_dist_version		
	print "The OS distribution information of the local machine:", host_linux_distro

	if hardware_platform != uname_list[4].lower():
		sys.exit("The specification requires " + hardware_platform + ", but the local machine is " + uname_list[4].lower() + "!\n")

	if os_type != uname_list[0].lower():
		sys.exit("The specification requires " + os_type + ", but the local machine is " + uname_list[0].lower() + "!\n")

""" Installation each software dependency specified in the software section of the specification
Parameter software_spec: the software section of the specification
Parameter sw_conn_cursor: the cursor of the connection to software.db
Parameter sandbox_dir: the sandbox directory
Parameter batch_type: the batch type
"""
def software_dependencies_installation(software_spec, sw_conn_cursor, sandbox_dir, batch_type):
	print "Installing software dependencies ..."
	#download each software dependency
	i = 1
	for item in software_spec:
		name = software_spec[item]['name']
		version = software_spec[item]['version']
		platform = software_spec[item]['platform']
		mountpoint = software_spec[item]['mountpoint']
		print "dependency_process(%s) start: %s" % (name, datetime.datetime.now())
		dependency_process(name, version, platform, mountpoint, sw_conn_cursor, sandbox_dir, batch_type)
		print "dependency_process(%s) end: %s" % (name, datetime.datetime.now())
		i = i + 1

	#if the OS distribution does not match, add the OS image into the dependency list of the application and download it into the local machine
	if linux_distro != host_linux_distro:
		print "The required linux distro specified in the specification is %s; the linux distro of the host machine is %s. The %s os images will be downloaded." % (linux_distro, host_linux_distro, linux_distro)
		name = dist_name
		version = dist_version	
		mountpoint = '/'	
		print "dependency_process(%s) start: %s" % (name, datetime.datetime.now())
		dependency_process(name, version, hardware_platform, mountpoint, sw_conn_cursor, sandbox_dir, batch_type)
		print "dependency_process(%s) end: %s" % (name, datetime.datetime.now())

	#if the batch_type is local, add cctools into the dependency list to guarantee parrot can be used to create sandbox for the worst case (No root access authority and no docker).
	if batch_type == "local":
		global local_cctools_path
		local_cctools_path = cctools_download(sandbox_dir, sw_conn_cursor)

""" Return the path of ld-linux.so within the downloaded os image dependency
"""
def dynamic_linker_path():
	#env_file is directly under the directory of the downloaded os image dependency
	if hardware_platform == "x86_64":
		return os.path.dirname(env_file) + "/lib64/ld-linux-x86-64.so.2"	
	else:
		return "undefined platform type"

""" Construct the docker volume parameters based on mount_dict
"""
def construct_docker_volume(sandbox_dir, input_dict):
	del mount_dict["/"] #remove "/" from the mount_dict to avoid messing the root directory of the host machine
	volume_parameters = ""
	if cvmfs_parrot == 1:
		cvmfs_data_path = os.path.dirname(sandbox_dir) + '/cache/cvmfs/cms.cern.ch/SITECONF/local'
		mount_dict[cvmfs_data_path] = '/cvmfs/cms.cern.ch/SITECONF/local'
	for key in mount_dict:
		volume_parameters = volume_parameters + " -v " + mount_dict[key] + ":" + key + " "

	for key in input_dict:
		volume_parameters = volume_parameters + " -v " + input_dict[key] + ":" + key + " "

	return volume_parameters

""" Get the path environment variable from envfile and add the mountpoints of software dependencies into it
"""
def import_path_env():
	with open(env_file, "rb") as file1:
		path_env = ''	
		for line in file1:
			if line[:5] == 'PATH=':
				path_env = line[5:-1]
				break
	for key in mount_dict:
		path_env = key + "/bin:" + path_env 
	return path_env

def transfer_env_para_to_docker(env_para_dict):
	other_envs = ''
	for key in env_para_dict:
		other_envs = other_envs + ' -e "' + key + '=' + env_para_dict[key] + '" '
	return other_envs


""" Construct the path environment from the mountpoints of software dependencies for the case where no rootfs needs to be constructed
"""
def real_path_env_extra():
	extra_path = ""
	for key in mount_dict:
		if key != '/': #the '/' should not be added into the path environment
			extra_path = '%s/bin:' % mount_dict[key]
	return extra_path

""" Construct the path environment from the mountpoints of software dependencies for the case where a separate rootfs needs to be constrcuted
"""
def rootfs_path_env_extra():
	extra_path = ""
	print "rootfs_path_env_extra(): mount_dict: ", mount_dict
	for key in mount_dict:
		print "key:", key
		if key != '/':
			extra_path += '%s/bin:' % key
	if local_cctools_path != "":
		extra_path += '%s/bin:' % local_cctools_path 
	return extra_path

""" Judge whether a user exists in /etc/passwd
"""
def judge_local_passwd():
	user_name = getpass.getuser()
	with open('/etc/passwd') as file1:
		for line in file1:
			if line[:len(user_name)] == user_name:
				print "%s is included in /etc/passwd!" % user_name
				return 'yes'
	return 'no'

""" Judge whether a group exists in /etc/group
"""
def judge_local_group():
	group_name = grp.getgrgid(os.getgid())[0]
	with open('/etc/group') as file1:
		for line in file1:
			if line[:len(group_name)] == group_name:
				print "%s is included in /etc/group!" % group_name
				return 'yes'
	return 'no'

""" Create the mountfile if parrot is used to create a sandbox for the application
Return the path of the mountfile
"""
def construct_mountfile(sandbox_dir, input_dict):
	mountfile = sandbox_dir + "/mountlist"		
	with open(mountfile, "wb") as file1:
		new_root = mount_dict["/"]
		file1.write("/ " + new_root + "\n")	
		del mount_dict["/"]
		file1.write(new_root + " " + new_root + "\n")	
		for key in mount_dict:
			file1.write(key + " " + mount_dict[key] + "\n")
			file1.write(mount_dict[key] + " " + mount_dict[key] + "\n")

		with open(common_mountfile, "rb") as file2:
			for line in file2:
				file1.write(line)
		file1.write(sandbox_dir + ' ' + sandbox_dir + '\n')
		file1.write('/etc/hosts /etc/hosts\n')
		file1.write('/etc/resolv.conf /etc/resolv.conf\n')
		#nd workstation uses NSCD (Name Service Cache Daemon) to deal with passwd, group, hosts services. Here first check whether the current uid and gid is in the /etc/passwd and /etc/group, if yes, use them. Otherwise, construct separate passwd and group files.
		existed_user = judge_local_passwd()
		if existed_user == 'yes':
			file1.write('/etc/passwd /etc/passwd\n')
		else:
			print 'the user is not included in the /etc/passwd!'
			with open('.passwd', 'w+') as passwd_file:
				passwd_file.write('%s:x:%d:%d:unknown:%s:%s\n' % (getpass.getuser(), os.getuid(), os.getgid(), sandbox_dir + '/' + getpass.getuser(), os.environ['SHELL']))
			file1.write('/etc/passwd %s/.passwd\n' % (sandbox_dir))

			with open('.__acl', 'w+') as acl_file:
				acl_file.write('%s rwlax\n' % getpass.getuser())

			#getpass.getuser() returns the login name of the user
			os.makedirs(getpass.getuser())

			global fake_passwd
			fake_passwd = 1

		existed_group = judge_local_group()
		if existed_group == 'yes':
			file1.write('/etc/group /etc/group\n')
		else:
			print 'the group is not included in the /etc/group!'
			with open('.group', 'w+') as group_file:
				group_file.write('%s:x:%d:%d\n' % (grp.getgrgid(os.getgid())[0], os.getgid(), os.getuid()))
			file1.write('/etc/group %s/.group\n' % (sandbox_dir))

			global fake_group
			fake_group = 1
	
		#add /var/run/nscd/socket into mountlist
		file1.write('/var/run/nscd/socket ENOENT\n')
		file1.write('/tmp /tmp\n')
		dest = "%s/cache/cctools-%s-%s/bin/parrot_run" % (os.path.dirname(sandbox_dir), hardware_platform, host_linux_distro)
		file1.write(dest + ' ' + dest + '\n')
		with open(special_files, "rb") as file3:
			for line in file3:
				file1.write(line)

		#add the input_dict into mountflie 
		for key in input_dict:
			file1.write(key + " " + input_dict[key] + "\n")

	return mountfile

""" Add the input files into the mountfile
"""
def construct_inputs_to_mountfile(sandbox_dir, input_dict):
	mountfile = sandbox_dir + "/mountlist"		
	with open(mountfile, "wb") as file1:
		#add the input_dict into mountflie 
		for key in input_dict:
			file1.write(key + " " + input_dict[key] + "\n")
		for key in mount_dict:
			file1.write(key + " " + mount_dict[key] + "\n")
			file1.write(mount_dict[key] + " " + mount_dict[key] + "\n")
	return mountfile

""" Read env_file and save all the environment variables into a dictionary
Return: a dictionary which includes all the environment variables from env_file
"""
def construct_env(sandbox_dir):
	with open(env_file, "rb") as file1:
		env_dict = {}
		for line in file1:
			index = line.find("=")
			key = line[:index] 
			value = line[(index+1):-1] 
			env_dict[key] = value	
		env_dict['PWD'] = sandbox_dir
		return env_dict

""" Check whether a docker images exists on the local machine or not.
"""
def check_docker_image():
	docker_image_name = "%s-%s-%s" %(dist_name, dist_version, hardware_platform)
	cmd = 'docker images ' + docker_image_name
	p = subprocess.Popen(cmd, stdout = subprocess.PIPE, shell = True)
	p_status = p.wait()
	while True:
		line = p.stdout.readline()
		if line != '':
			line = line.rstrip()	
			if line[:len(docker_image_name)] == docker_image_name:
				return 'yes'
		else:
			return 'no'

""" Create a docker image based on the cached os image directory
"""
def create_docker_image(sandbox_dir):
	docker_image_name = "%s-%s-%s" %(dist_name, dist_version, hardware_platform)
	image_location = os.path.dirname(sandbox_dir) + '/cache/' + docker_image_name
	#docker container runs as root user, so use the owner option of tar command to set the owner of the docker image
	cmd = 'cd ' + image_location + '; tar --owner=root -c .|docker import - ' + docker_image_name + '; cd -'
	p = subprocess.Popen(cmd, stdout = subprocess.PIPE, shell = True)
	p_status = p.wait()

""" Construct directory mount list and file mount list for chroot. chroot requires the target mountpoint must be created within the chroot jail.
Return: a tuple includes the directory mount list and the file mount list
"""
def construct_chroot_mount_dict(sandbox_dir, output_dir, input_dict, need_separate_rootfs):
	chroot_mount_dict_dir = {}
	chroot_mount_dict_file = {}
	if need_separate_rootfs == 1:
		with open(common_mountfile) as file1:
			for line in file1:
				index = line.find(' ')
				item = line[:index]
				chroot_mount_dict_dir[item] = item	
	
		with open(special_files) as file2:
			for line in file2:
				index = line.find(' ')
				item = line[:index]
				if os.path.exists(item):
					chroot_mount_dict_file[item] = item	
	
	if cvmfs_parrot == 1:
		cctools_path = os.path.dirname(sandbox_dir) + '/cache/cctools-x86_64-' + host_linux_distro 
		chroot_mount_dict_dir[cctools_path] = cctools_path
		
	chroot_mount_dict_dir[sandbox_dir] = sandbox_dir
	chroot_mount_dict_dir[output_dir] = output_dir 
	for key in mount_dict:
		if key != '/':
			chroot_mount_dict_dir[mount_dict[key]] = key

	chroot_mount_dict_file['/etc/passwd'] = '/etc/passwd'
	chroot_mount_dict_file['/etc/group'] = '/etc/group'
	chroot_mount_dict_file['/etc/hosts'] = '/etc/hosts'
	chroot_mount_dict_file['/etc/resolv.conf'] = '/etc/resolv.conf'

	for key in input_dict:
		mode = os.lstat(input_dict[key]).st_mode
		if S_ISDIR(mode):
			chroot_mount_dict_dir[input_dict[key]] = key
		else:
			chroot_mount_dict_file[input_dict[key]] = key

	return (chroot_mount_dict_dir, chroot_mount_dict_file)

""" Create each target mountpoint under the cached os image directory, `mount --bind`
"""
def chroot_mount_bind(chroot_mount_dict_dir, chroot_mount_dict_file, sandbox_dir, need_separate_rootfs):
	if need_separate_rootfs == 1:
		os_image_name = "%s-%s-%s" %(dist_name, dist_version, hardware_platform)
		os_image_path = os.path.dirname(sandbox_dir) + '/cache/' + os_image_name
	else:
		os_image_path = '/'
	#mount --bind -o ro hostdir sandboxdir
	for key in chroot_mount_dict_dir:
		if key != '/cvmfs/cms.cern.ch':
			jaildir = '%s%s' % (os_image_path, chroot_mount_dict_dir[key]) 
			hostdir = key
			if not os.path.exists(jaildir):
				os.makedirs(jaildir)
			cmd = 'mount --bind -o ro %s %s' % (hostdir, jaildir)
			print cmd
			func_call(cmd)
		else:
			jaildir = '%s%s/cache/cvmfs/cms.cern.ch/SITECONF/local' % (os_image_path, os.path.dirname(sandbox_dir))
			hostdir = '%s/cache/cvmfs/cms.cern.ch/SITECONF/local' % (os.path.dirname(sandbox_dir))
			if not os.path.exists(jaildir):
				os.makedirs(jaildir)
			cmd = 'mount --bind -o ro %s %s' % (hostdir, jaildir)
			print cmd
			func_call(cmd)

	for key in chroot_mount_dict_file:
		jailfile = '%s%s' % (os_image_path, chroot_mount_dict_file[key]) 
		hostfile = key
		if not os.path.exists(jailfile):
			with open(jailfile, 'w+') as f:
				pass
		cmd = 'mount --bind -o ro %s %s' % (hostfile, jailfile)
		print cmd
		func_call(cmd)

""" Remove all the created target mountpoints within the cached os image directory
It is not necessary to change the mode of the output dir, because only the root user can use the chroot method.
"""
def chroot_post_process(chroot_mount_dict_dir, chroot_mount_dict_file, sandbox_dir, need_separate_rootfs):
	if need_separate_rootfs == 1:
		os_image_name = "%s-%s-%s" %(dist_name, dist_version, hardware_platform)
		os_image_path = os.path.dirname(sandbox_dir) + '/cache/' + os_image_name
	else:
		os_image_path = '/'
	print "chroot_post_process"
	print "chroot_mount_dict_dir:", chroot_mount_dict_dir
	print "chroot_mount_dict_file:", chroot_mount_dict_file

	#chroot_mount_dict_file must be processed ahead of chroot_mount_dict_dir, because we can not umount a directory if there is another mountpoints created for files under it.
	for key in chroot_mount_dict_file:
		jailfile = '%s%s' % (os_image_path, chroot_mount_dict_file[key]) 
		if os.path.exists(jailfile):
			cmd = 'umount -f %s' % (jailfile)
			print cmd
			func_call(cmd)

	for key in chroot_mount_dict_dir:
		if key == '/cvmfs/cms.cern.ch':
			jaildir = '%s%s/cache/cvmfs/cms.cern.ch/SITECONF/local' % (os_image_path, os.path.dirname(sandbox_dir))
		else:	
			jaildir = '%s%s' % (os_image_path, chroot_mount_dict_dir[key]) 
		if os.path.exists(jaildir):
			cmd = 'umount -f %s' % (jaildir)
			print cmd
			func_call(cmd)
			#remove all the empty ancestor directory
			parent_dir = jaildir
			mode = os.lstat(parent_dir).st_mode
			if S_ISDIR(mode):
				print parent_dir
				while len(os.listdir(parent_dir)) == 0:
					os.rmdir(parent_dir)
					parent_dir = os.path.dirname(parent_dir)
					print parent_dir

""" Run user task directory on the host's rootfs
"""
def execute_task_host_rootfs(sandbox_dir, env_dict, batch_type):
	task_file.close()
	parrot_submit_file.close()
	#need to construct the local.sh
	if cvmfs_parrot == 1:
		cmd = "/bin/sh " + sandbox_dir + "/local.sh"
		print "env_dict: ", env_dict
		#cmd = 'pwd; env; cat cms_cmd1'
		func_call_output_withenv(cmd, env_dict, sandbox_dir)
		print "%s is done " % cmd
	else:
		if batch_type == 'local':
			cmd = "parrot_run " + user_cmd[0]
		elif batch_type == 'chroot':
			cmd = 'chroot / /bin/sh -c "cd %s; %s"' %(sandbox_dir, user_cmd[0]) 
		print "cmd:", cmd
		func_call_output_withenv(cmd, env_dict, sandbox_dir)


""" Run user's task with the help of the sandbox techniques, which currently inculde chroot, parrot, docker.
"""
def workflow_repeat(sandbox_dir, batch_type, output_dir, sw_conn_cursor, input_dict, env_para_dict):
	#sandbox_dir will be the home directory of the sandbox
	os.chdir(sandbox_dir)
	#at this point, all the software should be under the cache dir, all the mountpoint of the software should be in mount_dict
	if batch_type == "chroot":
		print "chroot ******"
		if need_create_rootfs == 1:
			#traverse the common-mountlist file to create the mountpoint and mount --bind -o ro
			#traverse the special file to create the mountpoint and mount --bind
			#remount /etc/passwd /etc/group /etc/hosts /etc/resolv.conf
			print "local-chroot rootfs construct and env setting start: %s" % datetime.datetime.now()
			(chroot_mount_dict_dir, chroot_mount_dict_file) = construct_chroot_mount_dict(sandbox_dir, output_dir, input_dict, 1)
			chroot_mount_bind(chroot_mount_dict_dir, chroot_mount_dict_file, sandbox_dir, 1)
			#redirect outputdir
			env_dict = construct_env(sandbox_dir)
			extra_path = rootfs_path_env_extra()
			env_dict['PATH'] = '%s:%s' % (env_dict['PATH'], extra_path[:-1])
			for key in env_para_dict:
				env_dict[key] = env_para_dict[key]
			print "local-chroot rootfs construct and env setting end: %s" % datetime.datetime.now()

			#run user_cmd
			os_image_name = "%s-%s-%s" %(dist_name, dist_version, hardware_platform)
			os_image_path = os.path.dirname(sandbox_dir) + '/cache/' + os_image_name
			if cvmfs_parrot == 1:
				task_file.close()
				parrot_submit_file.close()
				sub_cmd = "/bin/sh " + sandbox_dir + "/local.sh"
				#cmd = 'env; strace -o strace.log chroot %s /bin/sh -c "cd %s; %s"' % (os_image_path, sandbox_dir, sub_cmd)
				cmd = 'env; strace -o strace.log chroot %s /bin/sh -c "cd %s; %s"' % (os_image_path, sandbox_dir, user_cmd[0])
				#cmd = 'env; strace -o strace.log chroot %s /bin/sh -c "/root/umbrella_test/cache/cctools-x86_64-redhat5/bin/parrot_run -l /lib64/ld-linux-x86-64.so.2 -v; cd %s; ls"' % (os_image_path, sandbox_dir)
			else:
				cmd = 'chroot %s /bin/sh -c "cd %s; %s"' % (os_image_path, sandbox_dir, user_cmd[0])
			func_call_output_withenv(cmd, env_dict, sandbox_dir)
			print "local-chroot user_cmd execute end: %s" % datetime.datetime.now()
			
			#post-process to clean up
			chroot_post_process(chroot_mount_dict_dir, chroot_mount_dict_file, sandbox_dir, 1)
			print "local-chroot post processing end: %s" % datetime.datetime.now()
		else:
			print "chroot does not seprate rootfs"
			(chroot_mount_dict_dir, chroot_mount_dict_file) = construct_chroot_mount_dict(sandbox_dir, output_dir, input_dict, 0)
			chroot_mount_bind(chroot_mount_dict_dir, chroot_mount_dict_file, sandbox_dir, 0)
			env_dict = os.environ
			for key in env_para_dict:
				env_dict[key] = env_para_dict[key]
			extra_path = rootfs_path_env_extra()
			#print "extra_path: ", extra_path
			env_dict['PATH'] = '%s:%s' % (env_dict['PATH'], extra_path[:-1])
			#print "env_dict: ", env_dict 
			execute_task_host_rootfs(sandbox_dir, env_dict, 'chroot')
			chroot_post_process(chroot_mount_dict_dir, chroot_mount_dict_file, sandbox_dir, 0)
		#rename the sandbox_dir to output_dir
		os.rename(sandbox_dir, output_dir)
	elif batch_type == "docker":
		print "local-docker import os image directory into a docker image start: %s" % datetime.datetime.now()
		print "docker *******"
		print "uid: %s; gid: %d" % (os.getuid(), os.getgid())
		if need_create_rootfs == 1:
			if check_docker_image() == 'no':
				create_docker_image(sandbox_dir)
			print "local-docker import os image directory into a docker image end: %s" % datetime.datetime.now()

			#-v /home/hmeng/umbrella_test/output:/home/hmeng/umbrella_test/output 
			volume_output = " -v %s:%s " % (sandbox_dir, sandbox_dir)
			#-v /home/hmeng/umbrella_test/cache/git-x86_64-redhat5:/software/git-x86_64-redhat5/ 
			volume_parameters = construct_docker_volume(sandbox_dir, input_dict)

			#-e "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/software/git-x86_64-redhat5/bin" 
			path_env = import_path_env() 
			other_envs = transfer_env_para_to_docker(env_para_dict)
			docker_image_name = "%s-%s-%s" %(dist_name, dist_version, hardware_platform)
			#by default, docker executes user_cmd as the root user, `chown` is used to change the owner of the output dir to be the user who calls `umbrella`
			chown_cmd = 'chown -R %d:%d %s' % (os.getuid(), os.getgid(), sandbox_dir)
			#to count the post processing time, this cmd is split into two commands
			container_name = "container_%s" % docker_image_name
			cmd = 'docker run --name %s -i -t %s %s -e "PATH=%s" %s %s /bin/sh -c "cd %s; %s; %s"' % (container_name, volume_output, volume_parameters, path_env, other_envs, docker_image_name, sandbox_dir, user_cmd[0], chown_cmd)
			print cmd
			func_call(cmd)
			print "local-docker docker run user_cmd end: %s" % datetime.datetime.now()
			print "Rename the sandbox dir(%s) to the output directory(%s)" % (sandbox_dir, output_dir)
			os.rename(sandbox_dir, output_dir)
			cmd = 'docker rm %s' % container_name
			func_call(cmd)
			print "local-docker docker post processing end: %s" % datetime.datetime.now()
		else:
			#if a separate rootfs is not needed to execute the user's cmd, should forcely use other execution engine to run the user cmd.
			execute_task_host_rootfs(sandbox_dir) #need to midify
			os.rename(sandbox_dir, output_dir)
	elif batch_type == "local":
		print "local *******"
		print "local-parrot pre-processing start: %s" % datetime.datetime.now()
		print "uid: %s(%s); gid: %d(%s)" % (os.getuid(), getpass.getuser(), os.getgid(), grp.getgrgid(os.getgid())[0])

		if need_create_rootfs == 1:
			dynamic_linker = dynamic_linker_path()
			mountfile = construct_mountfile(sandbox_dir, input_dict)
			env_dict = construct_env(sandbox_dir)
			env_dict['PARROT_MOUNT_FILE'] = mountfile
			#here, setting the linker will cause strange errors.
			env_dict['PARROT_LDSO_PATH'] = dynamic_linker
			env_dict['USER'] = getpass.getuser()
			env_dict['HOME'] = sandbox_dir + '/' + getpass.getuser()
			extra_path = rootfs_path_env_extra()
			env_dict['PATH'] = '%s%s' % (extra_path, env_dict['PATH'])

			if parrot_localfile_flag == 0:
				parrot_submit_file.write("#!/bin/sh\n")
				#parrot_submit_file.write("echo $PATH\n")

				dest = "%s/cache/cctools-%s-%s" % (os.path.dirname(sandbox_dir), hardware_platform, host_linux_distro)
				#parrot_submit_file.write('env; which povray; pwd; ' + dest + '/bin/parrot_run -d all -o log ')
				parrot_submit_file.write(dest + '/bin/parrot_run ')
				parrot_submit_file.write(user_cmd[0] + '\n')
				#parrot_submit_file.write('povray +I4_cubes.pov +Oframe000.png +K.0  -H50 -W50\n')

#				print "local-parrot download successfulness testing start: %s" % datetime.datetime.now()
#				print "\nHere are some tests to illustrate the cctools and os images are downloaded********"
#				cmd = 'du -hs %s' % dest	
#				print cmd
#				print func_call(cmd)
#				
#				os_path = "%s/cache/redhat-%s-%s" % (os.path.dirname(sandbox_dir), dist_version, hardware_platform)
#				cmd = 'ls %s' % os_path 
#				print cmd
#				print func_call(cmd)
#				print "Here are some tests to illustrate the cctools and os images are downloaded********\n"
#				print "local-parrot download successfulness testing end: %s" % datetime.datetime.now()
				
			task_file.close()
			parrot_submit_file.close()
			print "local-parrot pre-processing end: %s" % datetime.datetime.now()
			cmd = "/bin/sh " + sandbox_dir + "/local.sh"
			func_call_output_withenv(cmd, env_dict, sandbox_dir)

			print "local-parrot user_cmd execution end: %s" % datetime.datetime.now()
		else:
			print "local does not seprate rootfs"
			mountfile = construct_inputs_to_mountfile(sandbox_dir, input_dict)
			env_dict = os.environ
			env_dict['PARROT_MOUNT_FILE'] = mountfile
			for key in env_para_dict:
				env_dict[key] = env_para_dict[key]
			extra_path = rootfs_path_env_extra()
			#print "extra_path: ", extra_path
			if 'PATH' not in env_dict: #if we run umbrella on Condor, Condor will not set PATH by default.
				env_dict['PATH'] = '/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin'
			env_dict['PATH'] = '%s%s' % (extra_path, env_dict['PATH'])
			#print "env_dict: ", env_dict 

			execute_task_host_rootfs(sandbox_dir, env_dict, 'local')
		print "Rename the sandbox dir(%s) to the output directory(%s)" % (sandbox_dir, output_dir)
		os.rename(sandbox_dir, output_dir)
		print "local-parrot post processing end: %s" % datetime.datetime.now()
	else:
		pass

""" the process of specification when condor execution engine is chosen
"""
def condor_process(specification, json_file, json_file_basename, sandbox_dir, output_dir, input_options):
	print "condor_process start: %s" % (datetime.datetime.now())
	if not os.path.exists(sandbox_dir):
		os.makedirs(sandbox_dir)
	if specification.has_key("hardware") and specification["hardware"]:
		initialize_execution_environment_parameters(specification["hardware"], specification["kernel"], specification["os"])
	else:
		sys.exit("this spec has no hardware section\n")

	print "condor_process environment checking end: %s" % (datetime.datetime.now())
	global condor_submit_path
	condor_submit_path = "condor_task.submit"

	input_files = input_options
	input_files = re.sub( '\s+', '', input_files).strip() #remove all the whitespaces within the inputs option
	if input_files == '':
			input_list_origin = ''
	else:
		input_list_origin = input_files.split(',') 

	transfer_inputs = ''
	new_input_options = ''
	for item in input_list_origin:
		index_equal = item.find('=')
		access_path = item[:index_equal]
		actual_path = item[(index_equal+1):]
		transfer_inputs += ', %s' % (actual_path)
		new_input_options += '%s=%s,' % (access_path, os.path.basename(actual_path))
	if new_input_options[-1] == ',':
		new_input_options = new_input_options[:-1]
	print transfer_inputs 
	print new_input_options

	global condor_submit_file
	condor_submit_file = open(condor_submit_path, "w+")
	condor_submit_file.write('universe = vanilla\n')
	condor_submit_file.write('executable = umbrella\n')
	condor_submit_file.write('arguments = "-c %s -l condor_umbrella -i \'%s\' -o condor_umbrella_output run \'%s\'"\n' % (json_file_basename, new_input_options, user_cmd[0]))
	condor_submit_file.write('transfer_input_files = umbrella, software.db, %s%s\n' % (json_file, transfer_inputs))
	condor_submit_file.write('transfer_output_files = condor_umbrella_output\n')
	if linux_distro == "redhat5":
		condor_submit_file.write('requirements = TARGET.Arch == "%s" && TARGET.OpSys == "%s" && TARGET.OpSysAndVer == "redhat6"\n' % (hardware_platform, os_type))
	else:
		condor_submit_file.write('requirements = TARGET.Arch == "%s" && TARGET.OpSys == "%s" && TARGET.OpSysAndVer == "%s"\n' % (hardware_platform, os_type, linux_distro))
	condor_submit_file.write('output = stdout\n') 
	condor_submit_file.write('error = stderr\n')
	condor_submit_file.write('should_transfer_files = yes\n')
	condor_submit_file.write('when_to_transfer_output = on_exit\n')
	condor_submit_file.write('log = task.log\n')
	condor_submit_file.write('queue\n')
	condor_submit_file.close()
	print "condor_process construct submit file end: %s" % (datetime.datetime.now())

	#submit condor job
	cmd = 'condor_submit condor_task.submit'
	func_call(cmd)
	print "condor_process submit job end: %s" % (datetime.datetime.now())
	#keep tracking whether condor job is done
	user_name = getpass.getuser()
	job_running = 1
	print "Waiting for the job is done ..."
	while job_running > 0:
		cmd = 'condor_q %s' % user_name
		p = subprocess.Popen(cmd, stdout = subprocess.PIPE, shell = True)
		p_status = p.wait()
		for line in p.stdout:
			index = line.find('jobs;')
			if index != -1:
				job_running = int(line[:(index-1)])
				if job_running == 0:
					break
		time.sleep(5)
	print "condor_process job execution end: %s" % (datetime.datetime.now())
	#check until the condor job is done, post-processing (put the output back into the output directory)
	os.rename('condor_umbrella_output', output_dir)
	print "condor_process post processing end: %s" % (datetime.datetime.now())

def ec2_process(specification, json_file, sw_conn_cursor, ssh_key, user_name, public_dns, output_dir, batch_type, input_list, input_options):
	if specification.has_key("hardware") and specification["hardware"]:
		initialize_execution_environment_parameters(specification["hardware"], specification["kernel"], specification["os"])
	else:
		sys.exit("this spec has no hardware section!\n")

	batch_set = ['ec2_level1', 'ec2_level2', 'ec2_level3', 'ec2_level4']
	if batch_type in batch_set:
		print "Start install python on the VM: %s" % datetime.datetime.now()
		cmd = 'ssh -o ConnectionAttempts=5 -o StrictHostKeyChecking=no -o ConnectTimeout=60 -i ' + ssh_key + ' ' + user_name + '@' + public_dns + ' \'yum -y install wget\''
		func_call(cmd)
		
		python_item = db_search("python", linux_distro, hardware_platform, sw_conn_cursor) 
		python_url = python_item[3]
		#get the url of python
		cmd = 'ssh -o ConnectionAttempts=5 -o StrictHostKeyChecking=no -o ConnectTimeout=60 -i ' + ssh_key + ' ' + user_name + '@' + public_dns + ' \'wget ' + python_url + '\''
		func_call(cmd)
		
		scheme, netloc, path, query, fragment = urlparse.urlsplit(python_url)
		python_url_filename = os.path.basename(path)
		cmd = 'ssh -o ConnectionAttempts=5 -o StrictHostKeyChecking=no -o ConnectTimeout=60 -i ' + ssh_key + ' ' + user_name + '@' + public_dns + ' \'tar zxvf ' + python_url_filename + '\''
		print "Finish install python on the VM: %s" % datetime.datetime.now()
		func_call(cmd)

	#scp umbrella, cmssw.json, software.db and input files to vm
	input_file_string = ''
	for input_file in input_list:
		input_file_string += input_file + ' ' 
	print "Start send umbrella software.db json file: %s" % datetime.datetime.now()
	cmd = 'scp -i ' + ssh_key + ' umbrella software.db ' + json_file + ' ' + input_file_string + ' ' + user_name + '@' + public_dns + ':' 
	func_call(cmd)
	print "Finish send umbrella software.db json file: %s" % datetime.datetime.now()

	print "Start Execution on the VM: %s" % datetime.datetime.now()
	python_dir = "python-%s-%s" % (hardware_platform, linux_distro) 
	cmd = 'ssh -o ConnectionAttempts=5 -o StrictHostKeyChecking=no -o ConnectTimeout=60 -i ' + ssh_key + ' ' + user_name + '@' + public_dns + ' \'' + python_dir + '/bin/python umbrella -c ' + json_file + ' -s specification -i "' + input_options + '" -o output run "' + user_cmd[0] + '"\''
	func_call(cmd)
	print "Finish Execution on the VM: %s" % datetime.datetime.now()

	print "Start Post Processing: %s" % datetime.datetime.now()
	cmd = 'ssh -o ConnectionAttempts=5 -o StrictHostKeyChecking=no -o ConnectTimeout=60 -i ' + ssh_key + ' ' + user_name + '@' + public_dns + ' \'tar cvzf output.tar.gz output\''
	func_call(cmd)

	cmd = 'scp -i ' + ssh_key + ' ' + user_name + '@' + public_dns + ':output.tar.gz .'
	func_call(cmd)

	if not os.path.exists(output_dir):
		os.makedirs(output_dir)

	cmd = 'tar zxvf output.tar.gz'
	func_call(cmd)
	os.rename('output', output_dir)
	print "Finish Post Processing: %s" % datetime.datetime.now()

""" Create the execution environment specified in the specification file and run the task on it.
"""
def specification_process(specification, sandbox_dir, behavior, sw_conn_cursor, cloud_conn_cursor, batch_type, output_dir, input_dict, env_para_dict):
	print "Specification_process start: %s" % datetime.datetime.now()
	if not os.path.exists(sandbox_dir):
		os.makedirs(sandbox_dir)

#copying input files is expensive, so mounting mechanism based on redirection the access_path to the actual_path is a better solution
	#copy all the input files into the sandbox	
#	for input_file in input_list: 
#		shutil.copy(input_file, sandbox_dir)

	#if batch_type is local, create the task.sh based on the user's requirements, download parrot, download SITEINFO, create a clean environment, do the experiment.
	#if batch_type is condor, create the task.sh based on the user's requirements, create condor submit file, submit condor job.
	if specification.has_key("hardware") and specification["hardware"] and specification.has_key("kernel") and specification["kernel"] and specification.has_key("os") and specification["os"]:
		initialize_execution_environment_parameters(specification["hardware"], specification["kernel"], specification["os"])
	else:
		sys.exit("this specification is not complete! You must have a hardware section, a kernel section and a os section!\n")

	execution_environment_check(sandbox_dir, batch_type)
	print "execution_environment_check end: %s" % datetime.datetime.now()

	print "software_dependencies_installation start: %s" % datetime.datetime.now()
	if specification.has_key("software") and specification["software"]:
		software_dependencies_installation(specification["software"], sw_conn_cursor, sandbox_dir, batch_type)
	else:
		print "this specification does not have software section!\n"
	print "software_dependencies_installation end: %s" % datetime.datetime.now()

	workflow_repeat(sandbox_dir, batch_type, output_dir, sw_conn_cursor, input_dict, env_para_dict)
	print "workflow_repeat end: %s" % datetime.datetime.now()

def dependency_check(dependency_list):
	for item in dependency_list:
		print "dependency check -- ", item, " "
		p = subprocess.Popen("which " + item, stdout = subprocess.PIPE, shell = True)
		(output, err) = p.communicate()
		p_status = p.wait()
		if p_status != 0:
			sys.exit("command `which(" + item + ")` failed. Please install " + item + " and ensure its directory be added into the PATH environment varibale.\n")

""" Get the minimum integer for the sandbox name
Parameter path: the path of the localdir
Return: the minimum integer which is not occupied by the current directories under the path
"""
def get_sandbox_id(path):
	i = 1
	while(os.path.exists(path + '/' + str(i))):
		i = i + 1
	return i

""" Start one VM instance through Amazon EC2 command line interface and return the instance id.
Parameter image_id: the Amazon Image Identifier
Return the id of the started instance
"""
def get_instance_id(image_id):
	cmd = 'ec2-run-instances ' + image_id + ' -t c3.large -k hmeng_key_1018 -g sg-24f96141 --associate-public-ip-address true'
	p = subprocess.Popen(cmd, stdout = subprocess.PIPE, shell = True)
	p_status = p.wait()
	while True:
		line = p.stdout.readline()
		if line != '':
			line = line.rstrip()	
			if line[:8] == 'INSTANCE':
				instance_id = line[9:19]
				return instance_id

""" Get the public dns of one VM instance from Amazon EC2, `ec2-run-instances` can not directly return the public dns of the instance, so this function is needed to check the result of `ec2-describe-instances` to obtain the public dns of the instance.
Parameter: the id of the VM instance
Return: the public dns of the instance
"""
def get_public_dns(instance_id):
	public_dns = ''
	while public_dns == None or public_dns == '' or public_dns == 'l':
		cmd = 'ec2-describe-instances ' + instance_id
		p = subprocess.Popen(cmd, stdout = subprocess.PIPE, shell = True)
		p_status = p.wait()
		while True:
			line = p.stdout.readline()
			if line != '':
				line = line.rstrip()	
				str1 = 'PRIVATEIPADDRESS' 
				if line[:16] == str1:
					index = line.find("ec2")
					public_dns = line[index:]
					break
	return public_dns

def main():
	parser = OptionParser(usage="usage: %prog [options] run \"command\"",
						version="%prog 1.0")
	parser.add_option("-c", "--config",
					action="store",
					default="spec.json",
					help="The specification json file. (By default: spec.json)")
	parser.add_option("-l", "--localdir",
					action="store", 
					default="./umbrella_test",
					help="The path of directory used for all the cached data and all the sandboxes. (By default: ./umbrella_test)",)
	parser.add_option("-i", "--inputs",
					action="store", 
					default='',
					help="The path of input files in the format of access_path=actual_path. i.e, -i '/home/hmeng/file1=/tmp/file2'. access_path must be consistent with the semantics of the provided command, actual_path can be relative or absolute. (By default: '')",)
	parser.add_option("-e", "--env",
					action="store", 
					default='',
					help="The environment variable. I.e., -e 'PWD=/tmp'. (By default: '')")
	parser.add_option("-o", "--output",
					action="store", 
					default="./umbrella_output",
					help="The path of output. (By default: ./umbrella_output)",)
	parser.add_option("-T", "--batch_type",
					action="store", 
					default="local",
					choices=['local', 'chroot', 'docker', 'condor',],
					help="Batch system type, which can be local, chroot, docker, condor. (By default: local)",)
	parser.add_option("--softwaredb",
					action="store", 
					default="./software.db",
					help="The path of software db. (By default: ./software.db)",)
	parser.add_option("--clouddb",
					action="store", 
					default="./cloud.db",
					help="The path of cloud db. (By default: ./cloud.db)",)

	(options, args) = parser.parse_args()

#	print 'Number of arguments:', len(sys.argv), 'arguments.'
#	print 'Argument List:', str(sys.argv)

	if not args:
		print "You must provide the behavior and the command!\n"
		parser.print_help()
		sys.exit()
	behavior = args[0]
	behavior_list = ["run"]
	if behavior not in behavior_list:
		print behavior + " is not supported by umbrella!\n"
		parser.print_help()
		sys.exit()

	#cmd = sys.argv[-1]

	batch_type = options.batch_type
	if batch_type == 'chroot':
		if getpass.getuser() != 'root':
			print 'You must be root to use chroot method.\n'
			parser.print_help()
			sys.exit()

	#get the absolute path of the localdir directory, which will cache all the data, and store all the sandboxes.
	#to allow the reuse the local cache, the localdir can be a dir which already exists.
	localdir = options.localdir
	localdir = os.path.abspath(localdir)

	#get a sandbox number, which will become the name of the sandbox directory
	global sandbox_no #modification of a global variable within a function must declare the variable using `global` directive
	sandbox_no = get_sandbox_id(localdir)
	sandbox_dir = localdir + '/' + str(sandbox_no)
#	print sandbox_dir

	#transfer options.env into a dictionary, env_para_dict
	env_para_dict = {}
	env_para = options.env
	if env_para is None:
		env_para_list = ''
	else:
		env_para = re.sub('\s+', '', env_para).strip()
		env_para_list = env_para.split(',')

	for item in env_para_list:
		index_equal = item.find('=')
		env_name = item[:index_equal]	
		env_value = item[(index_equal+1):]
		env_para_dict[env_name] = env_value

	json_file = options.config
	json_file_basename = os.path.basename(json_file)
	if not os.path.isfile(json_file):
		print "The specification json file does not exist! Please refer the -c option.\n"
		parser.print_help()
		sys.exit()
	with open(json_file) as json_data: #python 2.4 does not support this syntax: with open () as
		specification = json.load(json_data)

		#if the spec file has environ seciton, merge the variables defined in it into env_para_dict
		if specification.has_key("environ") and specification["environ"]:
			spec_env = specification["environ"]
			for key in spec_env:
				env_para_dict[key] = spec_env[key]

#	print env_para_dict

	global cwd_setting
	if 'PWD' in env_para_dict:
		cwd_setting = env_para_dict['PWD']
	else:
		cwd_setting = sandbox_dir

	#get the absolute path of each input file
	input_files = options.inputs
	input_files = re.sub( '\s+', '', input_files).strip() #remove all the whitespaces within the inputs option
	if input_files == '':
		input_list_origin = ''
	else:
		input_list_origin = input_files.split(',') 

	input_list = []
	input_dict = {}
	for item in input_list_origin:
		index_equal = item.find('=')
		access_path = item[:index_equal]
		actual_path = item[(index_equal+1):]
		if access_path[0] != '/':
			access_path = os.path.join(cwd_setting, access_path)
		actual_path = os.path.abspath(actual_path)
		input_dict[access_path] = actual_path
		input_list.append(actual_path) #get the absolute path of each input file and add it into input_list
#	print input_list
#	print input_dict

	#get the absolute path of each output file	
	output_dir = options.output
	output_dir = os.path.abspath(output_dir)

	if not os.path.exists(output_dir):
		os.makedirs(output_dir)
	elif len(os.listdir(output_dir)) != 0:
		sys.exit("%s is not empty! Please clean the output directory first or specify another directory!\n" % output_dir)
	else:
		pass
	#create a connection to the software.db, which includes the name, version, platform, storage location url, checksum, type of each dependency (OS, software, data)
	software_db = options.softwaredb
	if not os.path.exists(software_db):
		print 'the software db does not exist. Please refer the --softwaredb option.\n'
		parser.print_help()
		sys.exit()
	sw_conn = sqlite3.connect(software_db)
	sw_conn_cursor = sw_conn.cursor()

	#create a connection to the ec2.db, which includes the metadata information of ec2 resources.
	cloud_db = options.clouddb
	if not os.path.exists(cloud_db):
		print 'the cloud db does not exist. Please refer the --clouddb option.\n'
		parser.print_help()
		sys.exit()

	cloud_conn = sqlite3.connect(cloud_db)
	cloud_conn_cursor = cloud_conn.cursor()
	
	global user_cmd
	user_cmd = args[1:]

	if behavior in ["run"]:
		dependency_list = []
		dependency_check(dependency_list)

		user_name = 'root' #username who can access the VM instances from Amazon EC2
		ssh_key = 'hmeng_key_1018.pem' #the pem key file used to access the VM instances from Amazon EC2
		if batch_type == "ec2_level1":
			print "ec2_level 1 Start a ec2 VM: %s" % datetime.datetime.now()
			#get the instance id
			instance_id = get_instance_id('ami-7bdaa84b') #RHEL-6.5_GA-x86_64-9-Hourly2; for povray
			#get the public DNS of the instance_id
			public_dns = get_public_dns(instance_id)
			print "ec2_level 1 finishing start a ec2 VM: %s" % datetime.datetime.now()
			ec2_process(specification, json_file, sw_conn_cursor, ssh_key, user_name, public_dns, output_dir, batch_type, input_list, options.inputs)
		elif batch_type == "ec2_level2":
			public_dns = 'ec2-54-69-37-113.us-west-2.compute.amazonaws.com'
			ec2_process(specification, json_file, sw_conn_cursor, ssh_key, user_name, public_dns, output_dir, batch_type, input_list, options.inputs)
		elif batch_type == "ec2_level3":
			instance_id = get_instance_id('ami-d76a29e7') ##RHEL-5.10_GA-x86_64-8-Hourly2-GP2; for povray
			public_dns = get_public_dns(instance_id)
			ec2_process(specification, json_file, sw_conn_cursor, ssh_key, user_name, public_dns, output_dir, batch_type, input_list, options.inputs)
		elif batch_type == "ec2_level4":
			public_dns = 'ec2-54-69-219-28.us-west-2.compute.amazonaws.com'
			ec2_process(specification, json_file, sw_conn_cursor, ssh_key, user_name, public_dns, output_dir, batch_type, input_list, options.inputs)
		elif batch_type == "ec2_level5":
			public_dns = 'ec2-54-69-219-28.us-west-2.compute.amazonaws.com'
			ec2_process(specification, json_file, sw_conn_cursor, ssh_key, user_name, public_dns, output_dir, batch_type, input_list, options.inputs)
		elif batch_type == "condor":
			condor_process(specification, json_file, json_file_basename, sandbox_dir, output_dir, options.inputs)
		else:
			specification_process(specification, sandbox_dir, behavior, sw_conn_cursor, cloud_conn_cursor, batch_type, output_dir, input_dict, env_para_dict)

	#close the connections to databases
	sw_conn.close()
	cloud_conn.close()

if __name__ == "__main__":
	main()

#set sts=4 sw=4 ts=4 expandtab ft=python
